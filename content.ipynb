{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pragalvhasharma/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/nemoguardrails/rails/llm/config.py:527: RuntimeWarning: fields may not start with an underscore, ignoring \"_type\"\n",
      "  class RailsConfig(BaseModel):\n",
      "/Users/pragalvhasharma/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/nemoguardrails/rails/llm/config.py:527: RuntimeWarning: fields may not start with an underscore, ignoring \"_source\"\n",
      "  class RailsConfig(BaseModel):\n"
     ]
    }
   ],
   "source": [
    "colang_content = \"\"\"\n",
    "define user express greeting\n",
    "  \"hello\"\n",
    "  \"hi\"\n",
    "  \"whats up\"\n",
    "\n",
    "define bot express greeting\n",
    "  \"Hello there!! Can I help you today?\"\n",
    "\n",
    "define flow hello\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "\"\"\"\n",
    "yaml_content = \"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: gpt-3.5-turbo-1106\n",
    "# - type: embeddings\n",
    "#   engine: openai\n",
    "#   model: text-embedding-ada-002\n",
    "\"\"\"\n",
    "\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rails \u001b[38;5;241m=\u001b[39m \u001b[43mLLMRails\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/nemoguardrails/rails/llm/llmrails.py:204\u001b[0m, in \u001b[0;36mLLMRails.__init__\u001b[0;34m(self, config, llm, verbose)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_config()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Next, we initialize the LLM engines (main engine and action engines if specified).\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_llms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Next, we initialize the LLM Generate actions and register them.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m llm_generation_actions_class \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    208\u001b[0m     LLMGenerationActions\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcolang_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m LLMGenerationActionsV2dotx\n\u001b[1;32m    211\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/nemoguardrails/rails/llm/llmrails.py:355\u001b[0m, in \u001b[0;36mLLMRails._init_llms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    351\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provider \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support streaming.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm_config\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodels) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mprovider_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mregister_action_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/pydantic/main.py:1100\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:366\u001b[0m, in \u001b[0;36mChatOpenAI.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    350\u001b[0m client_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[1;32m    352\u001b[0m         values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_secret_value()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    363\u001b[0m }\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 366\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params)\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    368\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAsyncOpenAI(\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params\n\u001b[1;32m    370\u001b[0m     )\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "rails = LLMRails(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rails\u001b[38;5;241m.\u001b[39mgenerate_async(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rails' is not defined"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"hello\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Hello there!! Can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "# Passing a complete conversation\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hey there!\"}\n",
    "]\n",
    "res = await rails.generate_async(messages=messages)\n",
    "print(res)\n",
    "\n",
    "#Await is only for a .ipynd file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colang if else statement implemenation\n",
    "colang_content = \"\"\"\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello there!! Can I help you today?\"\n",
    "\n",
    "define bot personal greeting\n",
    "    \"Hello $username, nice to see you again!\"\n",
    "\n",
    "define flow hello\n",
    "    user express greeting\n",
    "    if $username\n",
    "        bot personal greeting\n",
    "    else\n",
    "        bot express greeting\n",
    "\"\"\"\n",
    "\n",
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")\n",
    "rails = LLMRails(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Hello there!! Can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "# When username is not given\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hey there!\"}\n",
    "]\n",
    "res = await rails.generate_async(messages=messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Hello Prag, nice to see you again!'}\n"
     ]
    }
   ],
   "source": [
    "# When given username\n",
    "messages = [\n",
    "    {\"role\": \"context\", \"content\": {\"username\": \"Prag\"}},\n",
    "    {\"role\": \"user\", \"content\": \"Hey there!\"},\n",
    "]\n",
    "res = await rails.generate_async(messages=messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain integration \n",
    "colang_content = \"\"\"\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello there!! Can I help you today?\"\n",
    "\n",
    "define bot personal greeting\n",
    "    \"Hello $username, nice to see you again!\"\n",
    "\n",
    "define flow hello\n",
    "    user express greeting\n",
    "    if $username\n",
    "      bot personal greeting\n",
    "    else\n",
    "        bot express greeting\n",
    "\n",
    "define flow\n",
    "    user ...\n",
    "    $answer = execute qa_chain(query=$last_user_message)\n",
    "    bot $answer\n",
    "\"\"\"\n",
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")\n",
    "rails = LLMRails(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m----> 7\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCatBank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms Furry Paws Credit Card offers 2\u001b[39;49m\u001b[38;5;132;43;01m% c\u001b[39;49;00m\u001b[38;5;124;43mashback on pet stores and 1\u001b[39;49m\u001b[38;5;132;43;01m% o\u001b[39;49;00m\u001b[38;5;124;43mn all other purchases.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeow Loans at CatBank come with a competitive 5\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m APR, subject to your catnip credit score.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEarn up to 4\u001b[39;49m\u001b[38;5;132;43;01m% i\u001b[39;49;00m\u001b[38;5;124;43mnterest with CatBank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms Kitty Savers account, with a minimum deposit of $500.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInvest in the Whisker Growth Fund with a minimum of $1,000 and enjoy purr-tential annual returns of 7\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpen a Feline Secure savings account with just $100; enjoy up to 3\u001b[39;49m\u001b[38;5;132;43;01m% i\u001b[39;49;00m\u001b[38;5;124;43mnterest with no monthly fees.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[1;32m     14\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful bot for our Bank. Only answer if your have got content in \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mOtherwise tell the user in a friendly way that you do not know and can not help with that.\u001b[39m\n\u001b[1;32m     16\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mAnswer here:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:736\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[1;32m    731\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m    732\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    733\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    734\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    735\u001b[0m     ):\n\u001b[0;32m--> 736\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    742\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:275\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:668\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:494\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    492\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 494\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    500\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:117\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 117\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m \u001b[43m_create_retry_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    121\u001b[0m     response \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:57\u001b[0m, in \u001b[0;36m_create_retry_decorator\u001b[0;34m(embeddings)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Wait 2^x * 1 second between each retry starting with\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# retry_min_seconds seconds, then up to retry_max_seconds seconds,\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# then retry_max_seconds seconds afterwards\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# retry_min_seconds and retry_max_seconds are optional arguments of\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# OpenAIEmbeddings\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry(\n\u001b[1;32m     49\u001b[0m     reraise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(embeddings\u001b[38;5;241m.\u001b[39mmax_retries),\n\u001b[1;32m     51\u001b[0m     wait\u001b[38;5;241m=\u001b[39mwait_exponential(\n\u001b[1;32m     52\u001b[0m         multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39membeddings\u001b[38;5;241m.\u001b[39mretry_min_seconds,\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39membeddings\u001b[38;5;241m.\u001b[39mretry_max_seconds,\n\u001b[1;32m     55\u001b[0m     ),\n\u001b[1;32m     56\u001b[0m     retry\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m---> 57\u001b[0m         retry_if_exception_type(\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241m.\u001b[39mTimeout)\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIError)\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIConnectionError)\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mRateLimitError)\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;241m|\u001b[39m retry_if_exception_type(openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mServiceUnavailableError)\n\u001b[1;32m     62\u001b[0m     ),\n\u001b[1;32m     63\u001b[0m     before_sleep\u001b[38;5;241m=\u001b[39mbefore_sleep_log(logger, logging\u001b[38;5;241m.\u001b[39mWARNING),\n\u001b[1;32m     64\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "vectorstore = Chroma.from_texts([\"CatBank's Furry Paws Credit Card offers 2% cashback on pet stores and 1% on all other purchases.\",\n",
    "                                 \"Meow Loans at CatBank come with a competitive 5% APR, subject to your catnip credit score.\",\n",
    "                                 \"Earn up to 4% interest with CatBank's Kitty Savers account, with a minimum deposit of $500.\",\n",
    "                                 \"Invest in the Whisker Growth Fund with a minimum of $1,000 and enjoy purr-tential annual returns of 7%.\",\n",
    "                                 \"Open a Feline Secure savings account with just $100; enjoy up to 3% interest with no monthly fees.\"],\n",
    "                                embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt_template = \"\"\"You are a helpful bot for our Bank. Only answer if your have got content in \"Context\".\n",
    "Otherwise tell the user in a friendly way that you do not know and can not help with that.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer here:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=rails.llm, # Use llm attached to rail instance\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rails.register_action(qa, name=\"qa_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n",
      "/Users/pragalvhasharma/Downloads/Prag GO to Documents/Comp Sci/MY Projects/NEMO Guardrails DEMO/env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `arun` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use ainvoke instead.\n",
      "  warn_deprecated(\n",
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"The minimum deposit for CatBank's Kitty Savers account is $500.\"}\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\",\"content\": \"Whats the minimum deposit for CatBank's Kitty Savers account?\"}]\n",
    "bot_message = await rails.generate_async(messages=history)\n",
    "print(bot_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n",
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Sorry, I can\\'t help with that. But I\\'m sure cows are \"moo-tivated\" to have a good laugh!'}\n"
     ]
    }
   ],
   "source": [
    "history = [{\"role\": \"user\",\"content\": \"Make a joke about a cow\"}]\n",
    "bot_message = await rails.generate_async(messages=history)\n",
    "print(bot_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_colang_content = \"\"\"\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello there!! Can I help you with your cat-related banking today?\"\n",
    "\n",
    "define bot personal greeting\n",
    "    \"Hello $username, nice to see you again! Hope your cat's doing well!\"\n",
    "\n",
    "define flow hello\n",
    "    user express greeting\n",
    "    if $username\n",
    "        bot personal greeting\n",
    "    else\n",
    "        bot express greeting\n",
    "\n",
    "define user ask about dogs\n",
    "    \"Can I get a loan for my dog?\"\n",
    "    \"Do you provide pet insurance for dogs?\"\n",
    "    \"What about doggie credit cards?\"\n",
    "\n",
    "define bot no dogs policy\n",
    "    \"Here at CatBank, we're all about cats! We don't offer services for dogs, but we can help with any cat-related banking needs.\"\n",
    "\n",
    "define flow dog policy\n",
    "    user ask about dogs\n",
    "    bot no dogs policy\n",
    "\n",
    "define user silly cat question\n",
    "    \"Can my cat open its own bank account?\"\n",
    "    \"Do you think cats are better bankers than humans?\"\n",
    "    \"Can I use a picture of my cat as my credit card design?\"\n",
    "\n",
    "define bot respond to silly cat question\n",
    "    \"While we love your cat's enthusiasm, only humans can open bank accounts. But we totally agree that cats would make amazing bankers, and yes, your credit card can definitely feature your cat's majestic portrait!\"\n",
    "\n",
    "define flow silly cat questions\n",
    "    user silly cat question\n",
    "    bot respond to silly cat question\n",
    "\n",
    "define user chit chat\n",
    "    \"What do you think about the latest movie?\"\n",
    "    \"Got any weekend plans?\"\n",
    "    \"Can you tell me a joke?\"\n",
    "\n",
    "define bot finance only\n",
    "    \"Sorry, I do cat related finance advice only!\"\n",
    "\n",
    "define flow chit chat\n",
    "    user chit chat\n",
    "    bot finance only\n",
    "\n",
    "define flow\n",
    "    user ...\n",
    "    $answer = execute qa_chain(query=$last_user_message)\n",
    "    bot $answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=new_colang_content\n",
    ")\n",
    "rails = LLMRails(config=config)\n",
    "rails.register_action(qa, name=\"qa_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Hello Markus, nice to see you again! Hope your cat's doing well!\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"context\", \"content\": {\"username\": \"Markus\"}},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "]\n",
    "res = await rails.generate_async(messages=messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Sorry, I do cat related finance advice only!'}\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\",\"content\": \"Can you tell me a joke?\"}]\n",
    "bot_message = await rails.generate_async(messages=messages)\n",
    "print(bot_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import DoctranPropertyExtractor\n",
    "from typing import Sequence, Any\n",
    "from langchain.schema import Document\n",
    "\n",
    "class CustomDoctranPropertyExtractor(DoctranPropertyExtractor):\n",
    "    def transform_documents(\n",
    "            self, documents: Sequence[Document], **kwargs: Any\n",
    "    ) -> Sequence[Document]:\n",
    "\n",
    "        \"\"\"Async method is currently broken (Document cant be used in async).\"\"\"\n",
    "        try:\n",
    "            from doctran import Doctran, ExtractProperty\n",
    "\n",
    "            doctran = Doctran(\n",
    "                openai_api_key=self.openai_api_key, openai_model=self.openai_api_model\n",
    "            )\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Install doctran to use this parser. (pip install doctran)\"\n",
    "            )\n",
    "\n",
    "        properties = [ExtractProperty(**property) for property in self.properties]\n",
    "        for d in documents:\n",
    "            doctran_doc = doctran.parse(content=d.page_content)\n",
    "            extracted_properties = doctran_doc.extract(\n",
    "                properties=properties).execute()\n",
    "\n",
    "            d.metadata[\"extracted_properties\"] = extracted_properties\n",
    "        return documents\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "async def get_bank_branch_address(branch):\n",
    "    properties = [\n",
    "        {\n",
    "            \"name\": \"mentioned_branch\",\n",
    "            \"description\": \"The bank branch mentioned in this query.\",\n",
    "            \"type\": \"string\",\n",
    "            \"required\": True\n",
    "        }\n",
    "    ]\n",
    "    property_extractor = CustomDoctranPropertyExtractor(\n",
    "        properties=properties, openai_api_model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    documents = [Document(page_content=branch)]\n",
    "    extracted_documents = property_extractor.transform_documents(\n",
    "        documents=documents\n",
    "    )\n",
    "    print(\"EXTRACTED_DOCS: \", extracted_documents)\n",
    "    branch_name = extracted_documents[0].metadata['extracted_properties'].transformed_content\n",
    "\n",
    "    branch_addresses = {\n",
    "        \"Central CatBank\": \"123 Feline Street, Cat City\",\n",
    "        \"CatBank North\": \"456 Purr Avenue, Kitty Corner\",\n",
    "        \"CatBank South\": \"789 Whisker Way, Tabby Town\",\n",
    "        \"Downtown CatBank\": \"101 Claw Circle, Meow Metro\"\n",
    "    }\n",
    "\n",
    "    address = branch_addresses.get(branch_name)\n",
    "    if address:\n",
    "        return f\"The address for {branch_name} is {address}.\"\n",
    "    else:\n",
    "        return \"Branch not found.\"\n",
    "\n",
    "\n",
    "\n",
    "function_colang_content = \"\"\"\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello there!! Can I help you with your cat-related banking today?\"\n",
    "\n",
    "define bot personal greeting\n",
    "    \"Hello $username, nice to see you again! Hope your cat's doing well!\"\n",
    "\n",
    "define flow hello\n",
    "    user express greeting\n",
    "    if $username\n",
    "        bot personal greeting\n",
    "    else\n",
    "        bot express greeting\n",
    "\n",
    "define user ask about dogs\n",
    "    \"Can I get a loan for my dog?\"\n",
    "    \"Do you provide pet insurance for dogs?\"\n",
    "    \"What about doggie credit cards?\"\n",
    "\n",
    "define bot no dogs policy\n",
    "    \"Here at CatBank, we're all about cats! We don't offer services for dogs, but we can help with any cat-related banking needs.\"\n",
    "\n",
    "define flow dog policy\n",
    "    user ask about dogs\n",
    "    bot no dogs policy\n",
    "\n",
    "define user silly cat question\n",
    "    \"Can my cat open its own bank account?\"\n",
    "    \"Do you think cats are better bankers than humans?\"\n",
    "    \"Can I use a picture of my cat as my credit card design?\"\n",
    "\n",
    "define bot respond to silly cat question\n",
    "    \"While we love your cat's enthusiasm, only humans can open bank accounts. But we totally agree that cats would make amazing bankers, and yes, your credit card can definitely feature your cat's majestic portrait!\"\n",
    "\n",
    "define flow silly cat questions\n",
    "    user silly cat question\n",
    "    bot respond to silly cat question\n",
    "\n",
    "define user chit chat\n",
    "    \"What do you think about the latest movie?\"\n",
    "    \"Got any weekend plans?\"\n",
    "    \"Can you tell me a joke?\"\n",
    "\n",
    "define bot finance only\n",
    "    \"Sorry, I do cat related finance advice only!\"\n",
    "\n",
    "define flow chit chat\n",
    "    user chit chat\n",
    "    bot finance only\n",
    "\n",
    "define user inquire branch address\n",
    "    \"Wo finde ich die Central CatBank?\"\n",
    "    \"Wo ist CatBank North?\"\n",
    "    \"Kannst du mir sagen, wo CatBank South ist?\"\n",
    "    \"Wo ist Downtown CatBank?\"\n",
    "\n",
    "define flow inquire branch address\n",
    "    user inquire branch address\n",
    "    $address = execute get_bank_branch_address(branch=$last_user_message)\n",
    "    bot $address\n",
    "\n",
    "define flow\n",
    "    user ...\n",
    "    $answer = execute qa_chain(query=$last_user_message)\n",
    "    bot $answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "config = RailsConfig.from_content(\n",
    "  \tyaml_content=yaml_content,\n",
    "    colang_content=function_colang_content\n",
    ")\n",
    "rails = LLMRails(config=config)\n",
    "rails.register_action(qa, name=\"qa_chain\")\n",
    "rails.register_action(get_bank_branch_address, name=\"get_bank_branch_address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in LoggingCallbackHandler.on_chat_model_start callback: TypeError('can only concatenate list (not \"str\") to list')\n",
      "Error while execution get_bank_branch_address: 1 validation error for DoctranConfig\n",
      "openai_deployment_id\n",
      "  Field required [type=missing, input_value={'openai_model': 'gpt-3.5...enai_token_limit': 8000}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/missing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I'm sorry, an internal error has occurred.\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\",\"content\": \"Wo ist die Central CatBank\"}]\n",
    "bot_message = await rails.generate_async(messages=messages)\n",
    "print(bot_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
